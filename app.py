import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from langchain_huggingface import HuggingFacePipeline

# -----------------------------
# Funci√≥n para construir el LLM
# -----------------------------
def build_llm(hf_token, model_name="meta-llama/Meta-Llama-3-8B-Instruct"):
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=hf_token)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",
        torch_dtype=torch.float16,
        use_auth_token=hf_token
    )

    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=512,
        temperature=0.7,
        top_p=0.9
    )

    llm = HuggingFacePipeline(pipeline=pipe)
    return llm

# -----------------------------
# App en Streamlit
# -----------------------------
st.set_page_config(
    page_title="EDA + LLM con Llama 3",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.sidebar.title("üìä Men√∫ de opciones")

# Secci√≥n de carga de datos
st.sidebar.subheader("Carga de datos")
uploaded_file = st.sidebar.file_uploader("üìÇ Sube tu archivo CSV", type=["csv"])
hf_token = st.sidebar.text_input("üîë Ingresa tu Hugging Face Token", type="password")

# Columna izquierda men√∫ / derecha contenido
menu = st.sidebar.radio("Navegaci√≥n", ["EDA Autom√°tico", "An√°lisis con LLM"])

# -----------------------------
# Manejo de datos
# -----------------------------
if uploaded_file is not None:
    try:
        df = pd.read_csv(uploaded_file)

        # Convertir solo columnas que contengan "date" o "fecha" en datetime
        for col in df.columns:
            if "date" in col.lower() or "fecha" in col.lower():
                try:
                    df[col] = pd.to_datetime(df[col], errors="coerce")
                except Exception:
                    pass

        st.success("‚úÖ Datos cargados correctamente")
    except Exception as e:
        st.error(f"‚ùå Error al leer el archivo: {e}")
        df = None
else:
    df = None

# -----------------------------
# Secci√≥n de EDA Autom√°tico
# -----------------------------
if menu == "EDA Autom√°tico":
    if df is not None:
        st.header("üìä Exploratory Data Analysis (EDA)")

        st.subheader("Vista previa de los datos")
        st.dataframe(df.head())

        st.subheader("Resumen general")
        st.write(df.describe(include="all"))

        st.subheader("Tipos de datos")
        st.write(df.dtypes)

        # Gr√°fico de correlaci√≥n solo si hay m√°s de 1 variable num√©rica
        numeric_df = df.select_dtypes(include=["number"])
        if numeric_df.shape[1] > 1:
            st.subheader("Mapa de calor - Correlaci√≥n")
            corr = numeric_df.corr()
            fig, ax = plt.subplots(figsize=(10, 6))
            sns.heatmap(corr, annot=True, cmap="coolwarm", ax=ax)
            st.pyplot(fig)
        else:
            st.info("‚ö†Ô∏è No hay suficientes variables num√©ricas para calcular correlaci√≥n.")

    else:
        st.info("üìÇ Por favor carga un CSV para iniciar el an√°lisis.")

# -----------------------------
# Secci√≥n de an√°lisis con LLM
# -----------------------------
elif menu == "An√°lisis con LLM":
    st.header("ü§ñ An√°lisis con LLM (Llama 3)")

    if df is not None and hf_token:
        # Construcci√≥n del LLM
        try:
            llm = build_llm(hf_token)
            st.success("‚úÖ LLM cargado correctamente")
        except Exception as e:
            st.error(f"‚ùå Error al inicializar el modelo: {e}")
            llm = None

        if llm is not None:
            # Resumen de los datos para dar contexto
            resumen = f"""
            Dataset con {df.shape[0]} filas y {df.shape[1]} columnas.
            Columnas: {', '.join(df.columns)}.
            Tipos: {df.dtypes.to_dict()}
            """

            st.subheader("Hazle una pregunta al modelo")
            user_q = st.text_area("‚úçÔ∏è Escribe tu pregunta sobre los datos:")

            if st.button("Preguntar al LLM"):
                if user_q.strip():
                    prompt = f"""
                    Basado en el siguiente resumen del dataset:

                    {resumen}

                    Responde la siguiente pregunta del usuario de forma clara y breve:
                    {user_q}
                    """

                    try:
                        response = llm.invoke(prompt)
                        st.markdown("### üìå Respuesta del LLM")
                        st.write(response)
                    except Exception as e:
                        st.error(f"‚ùå Error al generar la respuesta: {e}")
                else:
                    st.warning("‚ö†Ô∏è Escribe una pregunta primero.")
    else:
        st.info("üìÇ Carga un CSV y proporciona tu Hugging Face Token para usar el LLM.")

# -----------------------------
# Footer
# -----------------------------
st.markdown("---")
st.markdown("üí° App desarrollada con ‚ù§Ô∏è usando Streamlit, LangChain y Hugging Face")
